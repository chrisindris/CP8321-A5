# -*- coding: utf-8 -*-
"""MNIST_CNN_Torch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQa7i4uLwhc3CpFcm_QxEdK6VbB2GjR7

# MNIST CNN in PyTorch

Followed from [this tutorial](https://nextjournal.com/gkoehler/pytorch-mnist).
"""

import time
from PyQt5.QtCore import *

import torch
import torchvision
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import torchsummary
import matplotlib.gridspec as gs

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from os import name

"""### Hyperparameters"""


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, 50)  # original: 320 x 50
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)


class WorkerThread(QThread):

    update_progress = pyqtSignal(list)

    def run(self):

        n_epochs = 3  # we can do more than 3 epochs.
        batch_size_train = 64
        batch_size_test = 1000
        learning_rate = 0.01
        momentum = 0.5
        log_interval = 10

        random_seed = 1
        torch.backends.cudnn.enabled = False
        torch.manual_seed(random_seed)

        """### Data"""

        train_loader = torch.utils.data.DataLoader(
            torchvision.datasets.MNIST('./files/', train=True, download=True,
                                       transform=torchvision.transforms.Compose([
                                           torchvision.transforms.ToTensor(),
                                           torchvision.transforms.Normalize(
                                               (0.1307,), (0.3081,))
                                       ])),
            batch_size=batch_size_train, shuffle=True)

        test_loader = torch.utils.data.DataLoader(
            torchvision.datasets.MNIST('./files/', train=False, download=True,
                                       transform=torchvision.transforms.Compose([
                                           torchvision.transforms.ToTensor(),
                                           torchvision.transforms.Normalize(
                                               (0.1307,), (0.3081,))
                                       ])),
            batch_size=batch_size_test, shuffle=True)

        examples = enumerate(test_loader)
        batch_idx, (example_data, example_targets) = next(examples)

        network = Net()
        optimizer = optim.SGD(network.parameters(), lr=learning_rate, momentum=momentum)

        train_losses = []
        train_counter = []
        test_losses = []
        test_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]

        def train(epoch):
            network.train()
            send_counter = 0

            for batch_idx, (data, target) in enumerate(train_loader):
                optimizer.zero_grad()
                output = network(data)
                loss = F.nll_loss(output, target)
                loss.backward()
                optimizer.step()

                if batch_idx % log_interval == 0:

                    if send_counter % 40 == 0:
                        print("----------------DISPLAY-------------------")

                        model_children = list(network.children())
                        self.update_progress.emit(model_children)

                        """
                        plt.figure(figsize=(20, 17))
                        img1 = model_children[3].weight.detach().numpy()
                        plt.imsave("./output/lin1_" + str(epoch) + "_" + str(send_counter) + ".png", img1)

                        plt.figure(figsize=(20, 17))
                        img2 = model_children[4].weight.detach().numpy()
                        plt.imsave("./output/lin2_" + str(epoch) + "_" + str(send_counter) + ".png", img2)

                        plt.figure(figsize=(20, 17))
                        for i, filt in enumerate(model_children[0].weight):
                            plt.subplot(3, 4, i + 1)  # 10 5x5 filters
                            plt.imshow(filt[0, :, :].detach().cpu().numpy())
                            plt.axis('off')
                            #plt.savefig("./output/conv1_" + str(epoch) + "_" + str(send_counter) + ".png")
                            plt.savefig("./output/conv1.png")
                            self.update_progress.emit("./output/conv1.png")

                        plt.figure(figsize=(20, 17))
                        for i, filt in enumerate(model_children[1].weight):
                            plt.subplot(4, 5, i + 1)  # 10 5x5 filters
                            plt.imshow(filt[0, :, :].detach().cpu().numpy())
                            plt.axis('off')
                            plt.savefig("./output/conv2_" + str(epoch) + "_" + str(send_counter) + ".png")
                        """


                    send_counter += 1
                    print(send_counter)

                    c1 = float(torch.norm(network.conv1.weight.data))
                    c2 = float(torch.norm(network.conv2.weight.data))
                    f1 = float(torch.norm(network.fc1.weight.data))
                    f2 = float(torch.norm(network.fc2.weight.data))

                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                        epoch, batch_idx * len(data), len(train_loader.dataset),
                               100. * batch_idx / len(train_loader), loss.item()))

                    print(
                        'Norms- conv1: {:.3f}, conv2: {:.3f}, fc1: {:.3f}, fc2: {:.3f}'.format(
                            c1, c2, f1, f2))

                    train_losses.append(loss.item())
                    train_counter.append(
                        (batch_idx * 64) + ((epoch - 1) * len(train_loader.dataset)))
                    torch.save(network.state_dict(), './model.pth')
                    torch.save(optimizer.state_dict(), './optimizer.pth')


        """### Test
        
        After each epoch of training, we will test the network to see the predictions after that much training.
        """

        def test():
            network.eval()
            test_loss = 0
            correct = 0
            with torch.no_grad():
                for data, target in test_loader:
                    output = network(data)
                    test_loss += F.nll_loss(output, target, reduction='sum').item()
                    pred = output.data.max(1, keepdim=True)[1]
                    correct += pred.eq(target.data.view_as(pred)).sum()
            test_loss /= len(test_loader.dataset)
            test_losses.append(test_loss)
            print('\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
                test_loss, correct, len(test_loader.dataset),
                100. * correct / len(test_loader.dataset)))

        test()  # How good is the model with the default parameters?
        for epoch in range(1, n_epochs + 1):
            train(epoch)
            test()

"""### Diagrams

#### Log-Likelihood versus Examples Seen

Let's plot the loss of the NN as it trains. The blue curve shows the loss on the training set. The red dots show the loss on the test set.

The training loss shows up as a blue curve whereas the test loss shows up as red dots since we evaluate the model on the training set 100 times per epoch, but only 1 time per epoch on the test set.
"""

"""
with torch.no_grad():
    output = network(example_data)
"""


"""
fig = plt.figure()
plt.plot(train_counter, train_losses, color='blue')
plt.scatter(test_counter, test_losses, color='red')
plt.legend(['Train Loss', 'Test Loss'], loc='upper right')
plt.xlabel('number of training examples seen')
plt.ylabel('negative log likelihood loss')
plt.show()
"""

"""#### Some examples

Let us see how the model does on a few images.
"""

"""
fig = plt.figure()
for i in range(6):
    plt.subplot(2, 3, i + 1)
    plt.tight_layout()
    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')
    plt.title("Prediction: {}".format(
        output.data.max(1, keepdim=True)[1][i].item()))
    plt.xticks([])
    plt.yticks([])
plt.show()
"""

"""#### Final weights

After some epochs of training, let's take a look at the weight matrices. These would also get stored into the results folder.
"""

"""
print(network.conv1.weight.size())
print(network.conv2.weight.size())
print(network.fc1.weight.size())
print(network.fc2.weight.size())
print(network.conv1.weight)
print(network.fc2.weight)
"""

"""#### Norms

Let's take a look at those final norms. Notice that they seem to be virtually the same as those printed out during the end of the last training epoch.
"""

"""
print(
    float(torch.norm(network.conv1.weight.data)),
    float(torch.norm(network.conv2.weight.data)),
    float(torch.norm(network.fc1.weight.data)),
    float(torch.norm(network.fc2.weight.data))
)

for param in network.parameters():
    print(type(param.data), param.size())

for name, param in network.named_parameters():
    if param.requires_grad:
        print(name)

print(torchsummary.summary(network, (1, 28, 28)))
"""
